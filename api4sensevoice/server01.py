
from fastapi import FastAPI, File, UploadFile, Request, HTTPException
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from starlette.exceptions import HTTPException as StarletteHTTPException
from starlette.status import HTTP_422_UNPROCESSABLE_ENTITY
from pydantic import BaseModel
from funasr import AutoModel
from pydub import AudioSegment
import asyncio
import numpy as np
import torch
import torchaudio
import io
import soundfile as sf
import argparse
import uvicorn
import time
# import logging
from app.logging_config import get_logger

# Set up logging
# logging.basicConfig(level=logging.ERROR)
# logger = logging.getLogger(__name__)

logger = get_logger(__name__)


emo_dict = {
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
}

event_dict = {
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|Cry|>": "ğŸ˜­",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ¤§",
}

emoji_dict = {
    "<|nospeech|><|Event_UNK|>": "â“",
    "<|zh|>": "",
    "<|en|>": "",
    "<|yue|>": "",
    "<|ja|>": "",
    "<|ko|>": "",
    "<|nospeech|>": "",
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
    "<|Cry|>": "ğŸ˜­",
    "<|EMO_UNKNOWN|>": "",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ˜·",
    "<|Sing|>": "",
    "<|Speech_Noise|>": "",
    "<|withitn|>": "",
    "<|woitn|>": "",
    "<|GBG|>": "",
    "<|Event_UNK|>": "",
}

lang_dict = {
    "<|zh|>": "<|lang|>",
    "<|en|>": "<|lang|>",
    "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>",
    "<|ko|>": "<|lang|>",
    "<|nospeech|>": "<|lang|>",
}

emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {"ğŸ¼", "ğŸ‘", "ğŸ˜€", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·", }


def format_str(s):
    for sptk in emoji_dict:
        s = s.replace(sptk, emoji_dict[sptk])
    return s


def format_str_v2(s):
    sptk_dict = {}

    for sptk in emoji_dict:
        sptk_dict[sptk] = s.count(sptk)
        s = s.replace(sptk, "")
        emo = "<|NEUTRAL|>"
    for e in emo_dict:
        if sptk_dict[e] > sptk_dict[emo]:
            emo = e
    for e in event_dict:
        if sptk_dict[e] > 0:
            s = event_dict[e] + s
            s = s + emo_dict[emo]

    for emoji in emo_set.union(event_set):
        s = s.replace(" " + emoji, emoji)
        s = s.replace(emoji + " ", emoji)

    return s.strip()


def format_str_v3(s):

    def get_emo(s):
        return s[-1] if s[-1] in emo_set else None

    def get_event(s):
        return s[0] if s[0] in event_set else None

    s = s.replace("<|nospeech|><|Event_UNK|>", "â“")

    for lang in lang_dict:
        s = s.replace(lang, "<|lang|>")

    s_list = [format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
    new_s = " " + s_list[0]
    cur_ent_event = get_event(new_s)

    for i in range(1, len(s_list)):

        if len(s_list[i]) == 0:
            continue
        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:
            s_list[i] = s_list[i][1:]
        # else:
        cur_ent_event = get_event(s_list[i])
        if get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):
            new_s = new_s[:-1]
        new_s += s_list[i].strip().lstrip()
    new_s = new_s.replace("The.", " ")
    return new_s.strip()


app = FastAPI()

# è®¾ç½®è·¨åŸŸä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=[""],  # å…è®¸æ‰€æœ‰æ¥æºï¼Œå¯ä»¥æ ¹æ®éœ€è¦æŒ‡å®šç‰¹å®šçš„åŸŸå
    allow_credentials=True,
    allow_methods=[""],  # å…è®¸æ‰€æœ‰æ–¹æ³•
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´
)

# Initialize the model outside the endpoint to avoid reloading it for each request
model = AutoModel(model="iic/SenseVoiceSmall",
                      vad_model="fsmn-vad",
                      vad_kwargs={"max_single_segment_time": 300000},
                      trust_remote_code=True,
                      device="cuda:0",
                      )


def transcribe_with_timing(*args, **kwargs):
    start_time = time.time()
    result = model.generate(*args, **kwargs)
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"è½¬å½•è€—æ—¶: {elapsed_time:.2f} ç§’")
    return result, elapsed_time


@app.exception_handler(Exception)
async def custom_exception_handler(request: Request, exc: Exception):
    logger.error("Exception occurred", exc_info=True)
    if isinstance(exc, HTTPException):
        status_code = exc.status_code
        message = exc.detail
        data = ""
    elif isinstance(exc, RequestValidationError):
        status_code = HTTP_422_UNPROCESSABLE_ENTITY
        message = "Validation error: " + str(exc.errors())
        data = ""
    else:
        status_code = 500
        message = "Internal server error: " + str(exc)
        data = ""

    return JSONResponse(
        status_code=status_code,
        content=TranscriptionResponse(
            code=status_code,
            msg=message,
            data=data
        ).model_dump()
    )


# Define the response model
class TranscriptionResponse(BaseModel):
    code: int
    msg: str
    data: str


@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(file: UploadFile = File(...)):
    try:
        file.file.seek(0)
        file_content = await file.read()  # å¼‚æ­¥è¯»å–æ–‡ä»¶å†…å®¹
        print(f"[DEBUG] UploadFile Object is {file}")

        if file_content.startswith(b'RIFF'):  # å¦‚æœæ–‡ä»¶ä»¥RIFFå¼€å¤´ï¼Œè¯´æ˜æ˜¯WAVæ ¼å¼
            input_wav, sr = sf.read(io.BytesIO(file_content), dtype=np.int16)
            bit_depth = sf.info(io.BytesIO(file_content)).subtype
            is16 = True if bit_depth == 'PCM_16' else False
            print(f"[DEBUG] éŸ³é¢‘æ ¼å¼ä¸ºwav")

        elif file_content.startswith(b'\x89\x50\x4E\x47\x0D\x0A\x1A\x0A'):  # å¦‚æœæ–‡ä»¶ä»¥89 50 4E 47 0D 0A 1A 0Aå¼€å¤´ï¼Œè¯´æ˜æ˜¯WebMæ ¼å¼
            input_wav, sr = torchaudio.load(io.BytesIO(file_content))  # ä½¿ç”¨torchaudioåº“è¯»å–WebMæ–‡ä»¶å†…å®¹å’Œé‡‡æ ·ç‡
            dtype = input_wav.dtype  # è·å–éŸ³é¢‘æ•°æ®ç±»å‹
            is16 = True if dtype == np.int16 else False  # åˆ¤æ–­æ˜¯å¦ä¸º16ä½éŸ³é¢‘
            print(f"[DEBUG] éŸ³é¢‘æ ¼å¼ä¸ºwebm")
        elif file_content.startswith(b'\xFF\xF1') or file_content.startswith(b'\xFF\xF9') or file_content.startswith(
                b'\xFF\xFA') or file_content.startswith(b'\xFFxFB'):  # å¦‚æœæ–‡ä»¶ä»¥FF F1å¼€å¤´ï¼Œè¯´æ˜æ˜¯AACæ ¼å¼
            audio_segment = AudioSegment.from_file(io.BytesIO(file_content), format="aac")  # ä½¿ç”¨pydubåº“è¯»å–AACæ–‡ä»¶å†…å®¹
            input_wav = np.array(audio_segment.get_array_of_samples()).astype(np.int16)  # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è®¾ç½®ä¸º16ä½æ ¼å¼
            sr = audio_segment.frame_rate  # è·å–éŸ³é¢‘çš„é‡‡æ ·ç‡
            is16 = True  # è®¾ç½®ä¸º16ä½éŸ³é¢‘
            print(f"[DEBUG] éŸ³é¢‘æ ¼å¼ä¸ºaac")
        elif file_content.startswith(b'ID3'):  # å¦‚æœæ–‡ä»¶ä»¥FF E2å¼€å¤´ï¼Œè¯´æ˜æ˜¯MP3æ ¼å¼
            audio_segment = AudioSegment.from_file(io.BytesIO(file_content), format="mp3")  # ä½¿ç”¨pydubåº“è¯»å–MP3æ–‡ä»¶å†…å®¹
            input_wav = np.array(audio_segment.get_array_of_samples()).astype(np.int16)  # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è®¾ç½®ä¸º16ä½æ ¼å¼
            sr = audio_segment.frame_rate  # è·å–éŸ³é¢‘çš„é‡‡æ ·ç‡
            is16 = True  # è®¾ç½®ä¸º16ä½éŸ³é¢‘
            print(f"[DEBUG] éŸ³é¢‘æ ¼å¼ä¸ºmp3")
        else:
            raise HTTPException(status_code=400, detail="Unsupported audio format")

        # filename = (file.filename if file.filename else "test") + "." + suffix
        # with open(filename, "wb") as f:
            # f.write(file_content)


        if len(input_wav.shape) > 1:  # å¦‚æœéŸ³é¢‘æ˜¯ç«‹ä½“å£°ï¼Œå°†å…¶è½¬æ¢ä¸ºå•å£°é“
            input_wav = input_wav.mean(-1)

        if is16:  # å¦‚æœéŸ³é¢‘æ•°æ®ä¸æ˜¯æµ®ç‚¹æ•°ç±»å‹ï¼Œå°†å…¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°ç±»å‹å¹¶å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´
            input_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max

        if sr != 16000:  # å¦‚æœéŸ³é¢‘çš„é‡‡æ ·ç‡ä¸æ˜¯16000ï¼Œå°†å…¶é‡é‡‡æ ·ä¸º16000
            print(f"[DEBUG] éŸ³é¢‘é‡‡æ ·ç‡æ˜¯ {sr}  è¿›è¡Œé‡é‡‡æ ·")
            start_time = time.time()
            resampler = torchaudio.transforms.Resample(sr, 16000)  # åˆ›å»ºä¸€ä¸ªé‡é‡‡æ ·å™¨ï¼Œå°†é‡‡æ ·ç‡ä»sré‡é‡‡æ ·ä¸º16000
            input_wav_t = torch.from_numpy(input_wav).to(torch.float32)  # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶è®¾ç½®ä¸ºæµ®ç‚¹æ•°ç±»å‹
            input_wav = resampler(input_wav_t[None, :])[0, :].numpy()  # ä½¿ç”¨é‡é‡‡æ ·å™¨è¿›è¡Œé‡é‡‡æ ·ï¼Œå¹¶å°†ç»“æœè½¬æ¢å›NumPyæ•°ç»„
            end_time = time.time()
            elapsed_time = end_time - start_time
            print(f"é‡é‡‡æ ·è€—æ—¶: {elapsed_time:.2f} ç§’")


        async def generate_text():
            return await asyncio.to_thread(transcribe_with_timing,
                                           input=input_wav,
                                           cache={},
                                           language="auto",
                                           use_itn=True,
                                           batch_size=64)

        # Run the asynchronous function
        resp, elapsed_time = await generate_text()
        print(f"[DEBUG] è½¬å½•çš„åŸå§‹ç»“æœæ˜¯ {resp}")
        text = format_str_v3(resp[0]["text"])
        print(f'[DEBUG] æ ¼å¼åŒ–åçš„ç»“æœ res:{resp} text:{text}')

        # Create the response
        response = TranscriptionResponse(
            code=0,
            msg=f"success, è½¬å½•çš„æ—¶é—´ä¸º: {elapsed_time:.2f} ç§’",
            data=text
        )
    except Exception as e:
        logger.error("Exception occurred", exc_info=True)
        response = TranscriptionResponse(
            code=1,
            msg=str(e),
            data=" "
        )
    return JSONResponse(content=response.model_dump())


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the FastAPI app with a specified port.")
    parser.add_argument('--port', type=int, default=7000, help='Port number to run the FastAPI app on.')
    # parser.add_argument('--certfile', type=str, default='path_to_your_certfile', help='SSL certificate file')
    # parser.add_argument('--keyfile', type=str, default='path_to_your_keyfile', help='SSL key file')
    args = parser.parse_args()

    uvicorn.run(app, host="0.0.0.0", port=args.port)
